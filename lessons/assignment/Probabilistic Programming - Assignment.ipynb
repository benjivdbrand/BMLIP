{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [5SSD0] Probabilistic Programming - Assignment\n",
    "\n",
    "### Year: 2022-2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your name and student ID\n",
    "name = \n",
    "ID   = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we will go through the cycle of model specification, performance evaluation, critiqueing the model and revising it.\n",
    "\n",
    "![Figure taken from \"Build, Compute, Critique, Repeat: Data Analysis with Latent Variable Models\" [pdf](https://www.cs.columbia.edu/~blei/papers/Blei2014b.pdf)](figures/model-critique.png)\n",
    "_Figure taken from \"Build, Compute, Critique, Repeat: Data Analysis with Latent Variable Models\" ([pdf](https://www.cs.columbia.edu/~blei/papers/Blei2014b.pdf))_\n",
    "\n",
    "In questions 1 and 2, you will build a simple model, fit it to data and evaluate its performance on future data. You will find that its performance is not great. In question 3, you will improve the model in multiple ways. Finally, in question 4, you will do model selection based on free energy.\n",
    "\n",
    "The final questions will require knowledge from the last probabilistic programming session. But questions 1 and 2 can be done relatively early in the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg\n",
    "Pkg.activate(\".\")\n",
    "Pkg.instantiate();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CSV\n",
    "using DataFrames\n",
    "using LinearAlgebra\n",
    "using ProgressMeter\n",
    "using RxInfer\n",
    "using Plots\n",
    "default(label=\"\",\n",
    "        grid=false, \n",
    "        linewidth=3, \n",
    "        markersize=4,\n",
    "        guidefontsize=12, \n",
    "        margins=15Plots.pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem: Forecasting Air Quality\n",
    "\n",
    "Many Europeans suspect that the air quality in their city is declining. A [recent study](https://doi.org/10.1016/j.snb.2007.09.060) measured the air quality of a major city in North Italy using an electronic nose. The measurements were made in the middle of the city and reflect urban activity. We will inspect the specific chemical concentrations found and build a model to accurately predict CO for future time points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![https://www.theguardian.com/environment/2020/apr/07/air-pollution-linked-to-far-higher-covid-19-death-rates-study-finds](figures/air-milan-wide.png)\n",
    "\n",
    "Photograph taken by Claudio Furlan/LaPresse/Zuma Press/Rex/Shutterstock ([link](https://www.theguardian.com/environment/2020/apr/07/air-pollution-linked-to-far-higher-covid-19-death-rates-study-finds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data can be found here: https://archive.ics.uci.edu/ml/datasets/Air+Quality. I've done some pre-processing and selected the most important features. In this assignment we will infer parameters in a model of the data and predict air quality in the future. For that purpose, the data has been split into past and future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "past_data = DataFrame(CSV.File(\"data/airquality_past.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of data points\n",
    "N = 100;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the carbon monoxide measurements over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter(past_data[:,1], \n",
    "        past_data[:,2], \n",
    "        size=(900,300), \n",
    "        color=\"black\", \n",
    "        xlabel=\"time\", \n",
    "        ylabel=\"CO (ppm)\",\n",
    "        ylims=[400,2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Auto-regression\n",
    "\n",
    "We suspect that there is a temporal dependence in this dataset. In other words, the data changes relatively slowly over time and neighbouring data points end up being highly correlated. To exploit this correlation, we will build an _auto-regressive model_ of the form:\n",
    "\n",
    "$$ y_k = \\theta y_{k-1} + \\epsilon_k \\, , $$\n",
    "\n",
    "where the noise $\\epsilon_k$ is drawn from a zero-mean Gaussian with precision parameter $\\tau$: \n",
    "\n",
    "$$ \\epsilon_k \\sim \\mathcal{N}(0, \\tau^{-1}) \\, .$$\n",
    "\n",
    "Tasks:\n",
    "- [1pt] Specify the above equation as a probabilistic model in RxInfer, using $\\tau = 1.0$.\n",
    "- [1pt] Specify and execute an inference procedure to infer a posterior distribution for $\\theta$.\n",
    "- [1pt] Plot the inferred distribution over the interval $[0,\\ 2]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Predictions\n",
    "\n",
    "Now that we have inferred a posterior distribution for the coefficient, we can start making predictions. The data set also contains \"future data\" for which we want to make 1-step ahead predictions. The posterior predictive distribution for the next time step is:\n",
    "\n",
    "$$ p(y_{t+1} \\mid y_{t}, \\mathcal{D}) = \\int p(y_{t+1} \\mid \\theta, y_{t}) p(\\theta \\mid \\mathcal{D}) \\, \\mathrm{d}\\theta \\, , $$\n",
    "\n",
    "where $\\mathcal{D}$ refers to \"past data\" (used to infer the posterior distribution). To make 1-step ahead predictions, you will have to loop over the future data (i.e., `for t in 1:T`), plug in the current data point and compute the parameters of the posterior predictive distribution for the next data point. For the initial $y_t$, you may use the last entry of the \"past data\" set.\n",
    "\n",
    "Tasks:\n",
    "- [1pt] Compute the 1-step ahead predictions (mean and variance) for the \"future data\" set.\n",
    "- [1pt] Plot the predictions (variance in `ribbon=`) along with the actual future data (scatterplot).\n",
    "\n",
    "---\n",
    "\n",
    "Note that if you failed to infer a posterior distribution in the previous question, you can still answer this question using a standard normal, $p(\\theta) = \\mathcal{N}(0,1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "future_data = DataFrame(CSV.File(\"data/airquality_future.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = size(future_data,1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model critique\n",
    "\n",
    "Our model only considers extremely short term changes which are highly affected by noise. Furthermore, we only set the noise level $\\tau$ to $1.0$ but that was based on convenience, not on domain expertise or data. We are going to improve the model based on these two criteria. First, auto-regressive models can be extended further in the past to capture slower trends over time;\n",
    "\n",
    "$$ y_k = \\sum_{m=1}^{M} \\theta_m y_{k-m} + \\epsilon_k \\, ,$$\n",
    "\n",
    "where $M$ corresponds to model order. Secondly, we can put a prior probability distribution over $\\tau$ and infer a posterior $p(\\tau \\mid \\mathcal{D})$ simultaneously. To do that, you will have to specify the constraint $q(\\theta,\\tau) = q(\\theta)q(\\tau)$ in the variational inference procedure.\n",
    "\n",
    "Tasks:\n",
    "- [1pt] Extend the model with an order parameter $M$ and noise precision estimation.\n",
    "- [1pt] Infer the approximate posteriors for $\\theta$ and $\\tau$, for model order $M=3$.\n",
    "- [1pt] Visualize the posterior for the noise distribution $q(\\tau)$ (think carefully about its range).\n",
    "- [1pt] Visualize the 1-step ahead predictions (mean and variance) on the future data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of iterations of variational inference\n",
    "n_iters = 10;\n",
    "\n",
    "# Model order\n",
    "M = 5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model selection\n",
    "\n",
    "We now essentially have a different model for each value of $M$. Which is the best?\n",
    "\n",
    "Tasks:\n",
    "- [1pt] Compute the free energies for a given range of model orders and report the best performing one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model order range\n",
    "model_orders = [2,4,8,16,32];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "\n",
    "Before you submit the assignment, make sure your notebook runs! You can do that by going to the `Kernel` tab in the toolbar and pressing `Restart & Run All`. This is important! If your code doesn't run, we can't verify the correctness of your answer.\n",
    "\n",
    "When you're ready, head on over to Canvas and upload your notebook."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.8.1",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
